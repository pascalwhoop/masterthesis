The following chapter will describe the concepts and reasons behind various components needed to allow a broker to
leverage modern reinforcement learning tools in the \ac {PowerTAC} environment. Current state-of-the-art
algorithms for \ac {RL}, available in Python \citep{baselines}, are used. These leverage both the TensorFlow library
and, in one project, the Keras high-level abstraction library \citep{plappert2016kerasrl}.

%TODO what-if yes or no... make a choice dude
In general, a considerable amount of work was invested enabling communication between an agent
written in Python and the \ac {PowerTAC} systems which are Java based. The preliminary research and its results are summarized in
Section~\ref{sec:connecting_python_agents_to_powertac}. Because of this additional complexity, the practical part of the thesis was
restructured to allow for successful contribution to the \ac {RL} field by performing a form of \emph{what-if}
analysis in the wholesale market which is described in Section~\ref{sub:wholesale_market}. The Python environment has
been constructed in a way to allow for future developers to leverage it as a framework for developing a fully capable
agent that acts in all markets.

The overall architecture of the broker is composed of 5 main components: The communication abstraction, wholesale market
agent, balancing market agent, tariff market agent and demand predictor. In this implementation, only the wholesale
market and demand predictor are actively making decisions. Future researchers can make use of the components however. 
The architecture is visualized in Figure~\ref{fig:agentframework}.

While \citep{tactexurieli2016mdp} have defined the entire simulation as a \ac {POMDP} (although they interpret it as a
\ac {MDP} for ease of implementation) with all three markets integrated into one problem, I believe breaking the problem
into distinct sub-problems is a better approach as each of them can be looked at in separation and a learning algorithm
can be applied to improve performance without needing to consider potentially other areas of decision making. A
subsequent algorithm could then be trained to perform the same actions as one unified decision making system according
to the concepts of \emph{Curriculum Learning}\citep{matiisen2017teacher} and \emph{Transfer Learning}
\citep{parisotto2015actor}. Such a unified algorithm is not part of this work.
To justify this separation of concerns, I refer to the estimation of fitness for a given tariff in a given environment.
A tariffs' competitiveness in a given environment is independent of the wholesale or balancing trading strategy of the
agent since the customers do not care about the profitability of the agent or how often it receives balancing penalties.
While the broker might incur large losses if a tariff is too competitive (by offering prices that are below the
profitability line of the broker), such a tariff would theoretically be quiet competitive and should therefore be rated
as such. The question which of the tariffs to actually offer on the market is a separate problem, that balances
competitiveness against profitability. Similar arguments can be made for the other components.

I will first describe a number of tools used in the implementation as well as the preprocessing of the existing data
using both new and existing code. Afterwards I will describe the new communication architecture for non-java clients.
Finally I will explain the code behind the two implemented learning components, the demand prediction and the wholesale
trading agent.

%The overall architecture for the agent is composed of three key modules.First, the environment module, which hosts all known
%information about the environment of the broker. This is used by all learning components. Second, A communication module
%bridges the environment module and the \ac {PowerTAC} environment to hide communication overhead from the agent code,
%letting the learning components access the environment as if it was not remotely defined.
%Third, the agent components module holds all learning components such as the wholesale trader, the demand estimator
%and the tariff manager. In the scope of this thesis, only the demand estimator and the wholesale trader were implemented
%but the framework allows for the additional components to be easily implemented. The architecture is visualized in
%Figure~\ref{fig:agentframework}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\linewidth]{img/Agent.png}
    \caption{Broker framework for Python}
    \label{fig:agentframework}
\end{figure}


\section{Tools}

To develop the functionality of the agent, which is supposed to be mainly driven by deep learning technologies, a number
of state-of-the-art tools and frameworks were  used. These include
Keras and TensorFlow to allow for easy creation and adaption of the learning models,
\ac {GRPC} to communicate with the Java components of the competition and
\emph{Click} to create a CLI interface that allows the triggering of various components of the broker.

%TODO IF Kubernetes is used, I need to complete it. But what about CRIU?
%Kubernetes to easily scale several instances across the cloud.
%By transfering the components into the cloud, it is also
%possible to use tools such as Google Colab which allows access to a powerful cloud \ac {GPU} without costs
%\citep[]{GoogleColabOnline2018} .%TODO remove Google Inc in brackets


\subsection{TensorFlow and Keras}%
\label{sub:tensorflow_and_keras}

TensorFlow is a library developed by Google to facilitate machine learning algorithms. It can leverage both \ac {CPU}
and \ac {GPU} computing power which can significantly increase performance. It is Open Source, used in various
technologies and serves as a base technology for many higher level frameworks \citep{tensorflow2015-whitepaper}.

Keras is one of these higher level frameworks that focuses on \ac {NN}. It offers a intuitive \ac{API}, oriented towards
\ac {NN} terminology, to quickly develop and iterate on various \ac {NN} architectures. It integrates TensorFlow and its
accompanying UI Tensorboard, which visualizes training, network structure and activation patterns. It also supports
other base technologies beside TensorFlow, but these will not be discussed. A simple example for a 2 layer Dense \ac
{NN} written in Keras is shown in Listing~\ref{lst:kerasbasic}.


\begin{listing}
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{python}
from keras.layers import Dense

model.add(Dense(units=64, activation='relu', input_dim=100))
model.add(Dense(units=10, activation='softmax'))
model.compile(loss='categorical_crossentropy',
              optimizer='sgd',
              metrics=['accuracy'])
# x_train and y_train are Numpy arrays -- just like Scikit
model.fit(x_train, y_train, epochs=5, batch_size=32)
loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)
    \end{minted}
    \caption{Basic Keras 2 layer dense NN example}
    \label{lst:kerasbasic}
\end{listing}

\subsection{Click}%
\label{sub:click}

%TODO cite only name, year missing, not correct?
Click allows the creation of CLI interfaces in Python. Programms can be customised with parameters and options as well
as structured into subcommands and groups \citep{clickcli}. This allows for patterns such as \texttt{agent compete
--continuous} or \texttt{agent learn demand --model dense --tag v2}. An annotated function is shown in
Listing~\ref{lst:click_sample}.

\begin{listing}[h]
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{python}
@cli.command()
@click.option('--continuous', default=True)
def compete(continuous):
    """take part in a powertac competition"""
    import communication.powertac_communication_server as server
    server.serve()
    \end{minted}
    \caption{Click sample declaration}
    \label{lst:click_sample}
\end{listing}

\subsection{CRIU}%
\label{sub:criu}
\ac{CRIU} allows the freezing and storage of an application during runtime. This permits what is an equivalent of a fork
of the \ac {PowerTAC} simulation in a given point in time. Because \ac{CRIU} is also integrated into Docker, creating
containers for various components of the competition (i.e. server and brokers) and freezing all of them in a coordinated
manner is very helpful. This allows for two "what if" scenarios to play out at a given point in time where the results
can be compared \citep{criu}. A typical scenario for the technology is the live migration of running applications across
server infrastructures. In theory, a checkpoint of an application allows the perfect recreation of the application state
even after a complete reboot of the machine or the moving of the application to a different host with identical
environment settings.

\subsection{Docker}
\label{sub:docker}


Docker allows to create isolated, transferable images that include everything an application requires to run. A
container can be based on various distributions and many containers can run on a single server without much overhead.
\ac{VM} technologies are often compared to containers, but \ac{VM}s abstract on a different layer. A \ac{VM} simulates
an entire operating system on top of a layer called the hypervisor. Docker on the other hand only abstracts the
application layer, letting all containers run in the same kernel and therefore makes use of the existing ressources in a
more efficient way. Because \ac{CRIU} is integrated into Docker
\footnote{at the time of writing, CRIU support is experimental in Docker},
containers can be stored to disk using the \emph{checkpoint} feature.

\subsection{\ac {GRPC}}%
\label{sub:grpc}

\acf {GRPC} is a remote procedure call framework developed by Google Inc. It allows various languages and technologies to
communicate with each other through a common binary format. All communication can be encrypted via SSL, offering
security and authentication. Over-the-wire data representation can either be binary or \ac{JSON}
\citep[]{grpc}. The benefits over the
current implementation are described in Section~\ref{sub:grpc_based_communication}.


\subsection{MapStruct}%
\label{sub:mapstruct}

MapStruct offers transferring data between Java Objects of different classes. This problem is very common in large
software projects where domain objects may be out of the control of the developing team or based on external libraries.
If several components need to be integrated, translation is often necessary to adhere to the object structure required
by the library. MapStruct offers to generate otherwise manually created code based on best practices and naming
conventions. It is compile-time based, generating all code during compile time. This offers better error avoidance and
performance compared to alternatives that are reflection based
\citep[]{mapstruct}.
An example is given in Section~\ref{sub:implementing_the_communication_with_ac_grpc_and_mapstruct}.

\section{Preprocessing}

To learn from the large amount of data already available from previous simulations, parsing the state files provided by
the simulation is a reasonable approach to boost the ability of several parts of the agent to learn faster. One example
is the predictor of customer energy usage, as previous simulations offer large amounts of usage data that can be
analyzed. 

The original approach was to manually parse these files using python and to reconstruct the server state in python.
While this was successful for the demand component, I discovered the powertac-tools repository which holds similar tools
based on a combination of python and java components. This repository allows the creation of customer production and
consumption information in a comma separated file format. The initial approach to parse the files was therefore scrapped
and replaced with this prebuilt variant that makes use of the powertac-server source code. 

While the current demand prediction is solely based on historical demand, this can easily extended (as it has been in
the python only approach previously mentioned) with weather data, time information and up-to-date tariff information
\footnote{All preprocessing code has been deleted in commit
\href{https://github.com/pascalwhoop/broker-python/commit/c54ee7c05585d15462f40e2be6850343e8aea27a}{c54ee7c} in the
broker-python repository.}.


%The general architecture of the agent follows the idea of a core \emph{environment} module that holds all relevant data
%for a game. Tariffs, rates, customers, transactions and other data is stored in this module. Since the state files are
%based on events (they hold constructor parameters and method call parameters of previous server instances), these events
%need to be applied to the environment. To learn from these events, most modern frameworks require a training
%data-set and a label data-set. Therefore these events are first applied to the environment and at each timestep,
%relevant training samples are extracted. Therefore the overall structure of the translation from state files to training
%data is as follows:

%\begin{enumerate}
%    \item Iterate over all local state files
%    \item Iterate over lines in state file
%    \item Apply line to
%        current environment state
%    \item At each time-step, extract relevant samples
%    \item Store training data in separate local
%        %TODO separate vs online/streaming structured approach
%files \end{enumerate}
%
%The code linked to the process described above is part of the \texttt{util.state\_extractor} and
%\texttt{model.environment} modules. The tests in the \texttt{tests} module document the functionality.
%
%After the translation, the data is usually structured in a multi-dimensional array which can be read by numpy and
%processed with Keras. First, some preprocessing can be applied with scikit-learn to analyze the structure of the data as
%well as ensure the values that are fed to the \ac {NN} don't negatively impact the learning progress. The overall
%approach follows the recommendations of \citep{Goodfellow-et-al-2016}.


\section{Connecting Python agents to PowerTAC}%
\label{sec:connecting_python_agents_to_powertac}



To connect an agent based on Python to the \ac{PowerTAC} systems, a new adapter was developed. In early 2018, a simple bridge
was provided by John Collins, a member of the \ac {PowerTAC} team. It allowed external processes to communicate with the
system through a bridge via the provided sample-broker. All messages received by the broker were written to a First in
First Out pipe on the local file system and a second pipe was created to read messages from the external process. This
was the first approach towards opening up the simulation to other languages and development environments.


As I am interested in writing my Agent using certain frameworks which are mainly developed and maintained in Python and
because it is helpful to also allow access to the adapter via network interfaces (to allow for distributed execution of
the components in e.g. cloud or container environments), I needed to adapt this to allow network based access. In general
the following problems needed to be solved:

\begin{itemize}
    \item Java model classes should be reused if possible, automatically generating target language model
        definitions from the Java source code to avoid duplication of semantically identical information
    \item Permit future developers using even more languages (such as C, R or Go) with little effort
    \item Possibly lay the basis for a change of the communication technology of the entire simulation which is more
        language agnostic.
\end{itemize}

\subsection{Evaluating communication alternatives}%
\label{sub:evaluating_communication_alternatives}

After researching the current implementation and based on previous development experiences and current best practices,
the following three alternatives have been investigated in detail.


\subsubsection{\acs {XML} via \acs {GRPC} }%
\label{sub:xml_via_ac_grpc}


The first approach is quiet similar to the original bridge but instead of writing the \ac {XML} strings to the local file
system, they are passed to the final environment via \ac {GRPC} by simple messages that just serve as a wrapper for the
\ac {XML} string. While this is not elegant from a engineering perspective (\ac {GRPC} should be used on a method level
and messages should not contain other message formats as strings), it is simple and leads to quick results. A problem
is that the resulting \ac {XML} will then have to be parsed in the Python broker. Before the introduction of other
languages, the communication was basically an internal API and broker developers only needed to concern themselves with
the handling of the Java \texttt{handleMessage} method . Therefore, no formal descriptions for the structure of the \ac
{XML} messages exist. All \ac {XML} parsing would therefore be based on observable structures of the \ac {XML} which can
be extracted from the sample-broker logs and all model classes need to be rewritten. Furthermore, agents wanting to use
other programming languages would have to reimplement all of this again, with no reuse possible.

\subsubsection{True GRPC}%
\label{sub:grpc_based_communication}

A better but more complicated approach is based on \ac{GRPC} to transmit the messages between the Java sample-broker and
the final client, hooking into the \texttt{handleMessage} methods in the sample broker.
While previous developers have handled these messages in the Java environment, I
pass these messages to the ultimate environment by converting them into protobuf messages which are then sent to a
connected broker who implements corresponding handler methods in the target language.

The advantage of this approach is
that this theoretically allows the maintainers of the project to also adapt this approach for the Java clients in
general, massively reducing the communication overhead of \ac {XML} messages. The over-the-wire protocol is much more
efficient (as the data is sent in a binary format) and the message structure is clearly documented in the
\texttt{grpc\_messages.proto} file. When serializing a \texttt{Competition} object, \ac {XML} requires 48 kByte while
the \ac {GRPC} message is 14 kByte large, 70\% smaller.
\footnote{\url{https://github.com/pascalwhoop/grpc-adapter/blob/master/adapter/src/test/java/org/powertac/grpc/mappers/CompetitionMapperTest.java#L64}}.
When looking at the serialization and deserialization performance of \ac {XML} vs \ac {GRPC}, a comparison of 1000
iterations of each operation for each variant also shows a significant improvement. While the deserialization of \ac
{GRPC} is about 5x less performant ( 7444ms \ac {XML}, 1366ms \ac {GRPC}), the serialization is 44x times more
performant (1619ms \ac {XML}, 37ms \ac {GRPC})
\footnote{\url{https://github.com/pascalwhoop/grpc-adapter/blob/master/adapter/src/test/java/org/powertac/grpc/mappers/CompetitionMapperTest.java#L90}}.
This can be explained by the amount of string handling that \ac {XML} requires and on the other hand the fact that the
deserialization of \ac {GRPC} includes a mapping of the binary format into the proper Java object via MapStruct instead
of using reflection.


The disadvantage is the need to translate each \ac{POJO} into a protobuf message and
vice versa. This is however not different from the current XStream implementation which also requires the annotation of
class files in Java to declare which properties are serialized and included in the \ac {XML} strings. If the project
should adopt the \ac {GRPC} based communication, the \ac {GRPC} architecture will then allow the server to be addressed by
any of the supported languages \footnote{Which as of today are: C++, Java, Python, Go, Ruby, C\#, Node.js, PHP and
Dart}. Using MapStruct as a mapping tool also makes the mapping structured and by performing roundtrip tests of the
transformed elements, it can be assured that the transformations between \ac {GRPC} and \ac{POJO} perform as expected
\footnote{\url{https://github.com/pascalwhoop/grpc-adapter/blob/master/adapter/src/test/java/org/powertac/grpc/mappers/AbstractMapperTest.java#L54}}.



\subsubsection{JSON schema based communication}%
\label{sub:json_schema_based_communication}


A final approach is the generation of schema definitions from the Java model classes that are transmitted between the
brokers and the server. This formalizes the currently informal \ac {XML} \ac{API}. Generally, two human readable over-the-wire structures are reasonable: \ac {XML} and \ac{JSON}.
\ac {XML} messages can be formally defined using \ac {XML} Schemas and the \ac{JAXB} project
\footnote{\url{https://github.com/javaee/jaxb-v2}} offers to generate such schemas from Java class definitions. This
however did not succeed for the \ac {PowerTAC} model definitions which lead me to create a question on StackOverflow, a
discussion platform for programming questions. The resulting answer lead to the ultimate alternative which is the
generation of \ac {JSON} schemas which can then be converted into Python class files
\footnote{\url{https://stackoverflow.com/questions/49630662/convert-java-class-structures-to-python-classes/49777613\#49777613}}.
The choice of \ac {JSON} as the base communication protocol might also be intelligent as a future choice two reasons:
Firstly, it seems to be the more popular serialization protocol in comparison to \ac {XML} \citep{jsonxml} due to its
easy readability and because it is more data efficient. Secondly, \ac {GRPC} can also transmit data in \ac {JSON} form
and protobuf messages can easily be printed as \ac {JSON}, making both alternatives more interoperable
\footnote{\url{https://github.com/powertac/broker-adapter-grpc} }.

%Because the programming language is different from the supplied sample-broker, many of the domain objects need to be
%redefined and some code redeveloped. The classes in \ac {PowerTAC} which are transferred between the client and the
%server are all annotated so that the \ac {XML} serializer can translate between the \ac {XML}  and object variants without errors.
%This helps to recreate a similar functionality for the needed classes in the python environment. If the project was
%started again today, it might have been simpler to first define a set of message types in a language such as Protocol
%Buffers, the underlying technology of \ac {GRPC}, but because all current systems rely on \ac {JMI} communication, it is
%better to manually recreate these translators. The \ac {XML} parsing libraries provided by Python can be used to parse
%the \ac {XML} that is received.

\subsection{Implementing the communication with \ac {GRPC} and MapStruct}%
\label{sub:implementing_the_communication_with_ac_grpc_and_mapstruct}

After adapting the projects scope in response to the mid-thesis coordination with my supervisor, I chose the second
approach, the pure \ac {GRPC} solution. Because the focus will now be on the wholesale market, only a subset of messages
need to be mapped. This permits the implementation of a subset of message mappers between the \ac {GRPC} and
\ac {PowerTAC} entities, reducing the scope while retaining the benefits of a \ac {GRPC} implementation which generally
can be regarded as the best alternative.

Using MapStruct, all messages required for the wholesale learning component are mapped from the simulation core entities
to the \ac {GRPC} messages. To map classes, a mapper interface is created for each type. Most simple types can
automatically be mapped and don't require any adaption. All properties have been named the exact same way as the
properties of the data holding entities in the \ac {PowerTAC} environment, allowing MapStruct to deduce the
corresponding properties to map to. Some properties require custom initiation, more specifically those where the \ac
{PowerTAC} entities don't follow the bean specification for getters and setters. An example is given in
Listing~\ref{lst:mapperexample}. Mappings are defined with the \texttt{@Mappings(\{\})} annotation. Complex compositing
objects require the other needed Mappers to be defined in the \texttt{@Mapper(uses = \{...\})} annotation. Support for
Protocol Buffers in MapStruct is still fresh and many currently required lines of code may soon be redundant.

To ensure the mapping works as expected, the tests for the mapper classes perform a \emph{roundtrip test}. This takes a
Java class as commonly found in the simulation, converts it into \ac {XML} using the current XStream systems, then
performs a translation into \ac {GRPC} and back. Finally, this resulting object is serialized into \ac {XML} again and
both \ac {XML} strings are asserted to be equal. By doing this, many things, several things are tested at once: Is the
translation working as expected, i.e. does it retain all information of the original objects? Is the mapping of IDs to
objects still working as expected? Are any values such as dates or time values misrepresented? The roundtrip test allows
for a generic testing of all object types that covers a large number of possible errors.

\begin{listing}[]
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{java}
@Mapper(uses = {
    OrderbookMapper.BuilderFactory.class,
    InstantMapper.class,
    TimeslotMapper.class,
    OrderbookOrderMapper.class

},
    collectionMappingStrategy =
        CollectionMappingStrategy.ADDER_PREFERRED,
    nullValueCheckStrategy = ALWAYS)
public interface OrderbookMapper
    extends AbstractPbPtacMapper<PBOrderbook, Orderbook>
{

  OrderbookMapper INSTANCE =
    Mappers.getMapper(OrderbookMapper.class);

  @Mappings({})
  PBOrderbook.Builder map(Orderbook in);
  @Mappings({})
  Orderbook map(PBOrderbook in, @MappingTarget Orderbook out);

  //BuilderFactory helper class
}
    \end{minted}
    \caption{Mapper for Orderbook class}
    \label{lst:mapperexample}
\end{listing}

With an ability to translate Java objects into Protobuf messages, those messages now need to be transferred. \ac {GRPC}
offers the ability to transfer Protocol Buffer objects both as streams and as unary operations. The entire communication
overhead between the server and the client is abstracted away from the developer. The messages can therefore simply be
sent to the connected python broker code through the \ac {GRPC} adapter. The integration with the existing code is shown
in Listing~\ref{lst:handlemessageexample}.

\begin{listing}
    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{java}
public synchronized void handleMessage(Orderbook orderbook)
  {
    PBOrderbook msg = comm.converter.convert(orderbook);
    comm.marketStub.handlePBOrderbook(msg);
  }
    \end{minted}
    \caption{handleMessage example}
    \label{lst:handlemessageexample}
\end{listing}

On the Python side, the messages are now accepted and applied to the brokers knowledge base. This is encapsulated in the
env module of the broker as described before. Some messages may also be considered as action triggers and are therefore
shared with all interested components through the publish-subscribe event system. A message signaling a completed
time slot for example may trigger the broker to learn on the newly observed usage patterns, improve its predictions on
the expected usages of its customers and evaluate its next steps in the wholesale trading market. To perform such
actions however, the broker must first be able to compete in a competition.

\section{Creating Containers from competition components}
\label{sec:creating_containers_from_competition_components}

To run a competition on a local machine, one must install several components: Maven, Java 8 and all of the brokers as
well as ones own technology stack. If the scale of this set of components exceeds the local computation power available,
the stack needs to be moved to a machine in a server with sufficient computation power. While tools like Vagrant allow
the configuration and setup of environments to quickly allow new developers to start working with a set of tools in a
given project \citep{vagrant} , it requires virtual machines which have significant overhead in comparison to container
technologies. If the competition is abstracted into docker images, tools like Kubernetes or Docker Compose can quickly
instantiate a competition on any machine, given it has enough resources and a docker runtime installed \citep{docker}.
Containerized application infrastructures have recently become quiet popular. Amazon, Google and Microsoft are offering
services specifically tailored to host containerized applications and it is quiet easy to share created docker images
through the docker hub platform.

To create a Docker image for the server, the \texttt{Dockerfile} listed in Listing~\ref{lst:servertodocker} can be used
\footnote{All resources regarding the container technologies can be found under
\url{https://github.com/pascalwhoop/powertac-kubernetes}}.
It is also common practice to run the build in one container and move the created executable artifact into another
container that is solely responsible for executing said artifact. The \emph{alpine} image type is an extremely
light-weight Linux base that only requires about 5Mb of storage. Container images are therefore quiet light-weight in comparison
to virtual machines.

\begin{listing}[h]

    \begin{minted}[linenos,numbersep=5pt,frame=lines,framesep=2mm]{Dockerfile}
FROM openjdk:alpine
LABEL maintainer=pascalwhoop
LABEL name=powertac-server

WORKDIR /powertac
RUN mkdir data

COPY bootstrap-data.xml ./
COPY init.sh ./
COPY server.properties ./
#assumes a built server jar is in the target folder of this project
COPY target/server-jar-1.5.1-SNAPSHOT.jar server.jar

EXPOSE 8080 61616
#and start it up
CMD "/powertac/init.sh"
    \end{minted}
    \caption{Turning the current server snapshot into a docker image}
    \label{lst:servertodocker}
\end{listing}

This offers another advantage that may become increasingly attractive in the long-term: Tools like Kubernetes or Docker Swarm, both being open source enterprise level container management
software, seamlessly allow for the creation of 1, 10 or 1000 instances. OpenAI, a deep learning research company, has
successfully scaled Kubernetes to 2500 nodes to run their deep \ac{RL} learning systems \citep{openai2500}. As
previously mentioned, Docker also integrates \ac{CRIU} which is required for the creation of snapshots of competition
states.

%TODO implement redis as base for communication between components
%\subsection{Redis and component messaging}%
%\label{sub:redis_and_messaging}

\subsection{Inner Python communication}%
\label{sub:inner_python_communication}

Once the competition environment is running and messages start streaming into the python environment, the various
components need to be coordinated in an intelligent fashion. Event driven architectures are light-weight and offer
enough flexibility to coordinate the various components. The server may send a number of \texttt{TariffTransaction}
messages followed by a \texttt{TimeslotComplete} message that signals the demand estimator it may now calculate new
forecasts for any customer subscribed to the broker. Once the component has completed the task, instead of directly
calling another component such as the wholesale trader, it sends a signal via the event system so that any subscribed
component may react to the event.


%TODO END

%The components of the agent that have learning capabilities include:
%
%\begin{description}
%    %TODO will i still get to implement this? simply mimick an agents tariffs ... shouldn't be hard. The default tariff
%    %is already sent to the server from within Java, so that is also an option.
%    \item[Customer Market]: Generates actions in respect to the tariff market such as publishing,
%        adapting and revoking tariffs. While the component is expected to have a positive impact on the performance of
%        the broker, it was just implemented with a basic functionality of publishing the same tariffs as a selected
%        competitive broker, mimicking the competing brokers portfolio. It also creates usage predictions for a set of
%        customers for other components. Generally, the framework intends a tariff fitness evaluation as well as tariff
%        selection component that weighs both competitiveness and expected profitability.
%
%    \item[Wholesale Market]: Places bids and asks for energy in the periodic double
%        auction type market \citep{ketter2018powertac}. The component employs \ac {RL} techniques and uses the
%        predictions generated from the customer market component as an input describing the required capacity.
%
%    \item[Balancing Market]: The balancing market component has not been implemented, but it is part of the
%developed framework for possible future extension.  \end{description}

\section{Customer demand estimation}
\label{sub:customer_market}

%The customer market component covers the tariff market and the demand prediction tasks. In my implementation, the
%broker does not perform autonomous tariff creation but instead just publishes the default tariffs. The goal was to focus
%on the wholesale market and the demand prediction.


%TODO background?
%TODO not implemented
%The goal of the customer market is to get as many subscribers as possible for the most profitable tariffs the broker
%offers on the market. The tariffs offered in the market compete for the limited number of customers available and every
%customer must be subscribed to some tariff. The profitability of tariffs is limited by the base tariff which is offered
%by the simulation as a constant offering creating an upper bound on profitability.
%
%To succeed in the customer market, the agents needs to be able to generate tariffs that are competitive. This can be
%broken down into two subtasks: Generating valid tariffs and evaluating their competitiveness. A tariff can be verified
%by passing it to the \ac {PowerTAC} server which verifies the tariff. Hence, a \ac {RL} algorithm that is tasked with
%creating competitive tariffs can be given feedback by penalizing non-conclusive tariffs. An invalid tariff could be one
%that contains overlapping rates leading to an ambivalent status. The competitiveness of a tariff depends not only on the
%attributes of the tariff but also on the competition environment. If the broker only competes against the default
%tariffs, even many mediocre tariff offerings would perform well. In an environment with many competitors on the other
%hand, a tariff needs to be well designed to generate profits.
%
%The agents learning task for the customer market is therefore designed in the following way:
%
%\begin{enumerate} \item Learning to evaluate a tariffs competitiveness in relation to the competitive environment
%   through supervised learning on the historical state logs of previous competitions \item Running a \ac {RL}
%   algorithm which learns to choose parameters for tariffs that are valid and profitable in a given environment
%    %\item Learning to generate valid tariff specifications through a genetic algorithm strategy, penalizing invalid
%   %tariffs %TODO really, I go genetic?
%\end{enumerate}
%
%%TODO not yet actually realized, still applicable?
%\subsubsection{Tariff fitness learning} To learn the fitness of a tariff while considering its environment, supervised
%learning techniques can be applied. To do this, features need to be created from the tariffs specifications and its
%competitive environment. Similar work has been done by \citep{cuevas2015distributed} who discretized the tariff market
%in four variables describing the relationships between the competitors and their broker.
%
%For my broker, because \ac {NN} can handle a large state spaces, I create a more detailed description of the
%environment. I still have to ensure the number of input features is fixed though, so a simple copy of all competing
%tariffs is not a valid input for the environment description. Instead I create the following features from the tariff
%market:
%
%\begin{description} \item[Average Charge per hour of week Timeslot]: According to \\
%   \texttt{TariffEvaluationHelper.java}, customer models evaluate tariffs on an per-hour basis. This means they are
%   very precise in the evaluation of potential tariff alternatives (before the application of an irrationality
%   factor). Hence, a per-hour precision in the input is needed.  \item[Variance of Charge per hour of week
%   Timeslot] Variance of the tariffs charges per each timeslot in a week among all competitors.  \item[Average and
%   Variance of periodic payments] Description of the markets periodic payments landscape \item[Average and Variance
%   of one-time payments] Description of the markets one-time payments landscape \item[Average and Variance of
%   Up/Down regulation payments] 0 for tariffs without regulation capabilities \end{description}
%
%Because the \ac {PowerTAC} simulation does not return profits of brokers on a per-tariff basis and because the reasons
%for why a broker purchased a specific amount of energy on the wholesale market are not known, it is hard to put a
%profitability value on a brokers tariff if said broker offers more than one tariff on the market. Therefore the
%evaluation of the tariff does not include the profitability of the tariff but merely the competitiveness in regards to
%the attractiveness of the offer from the perspective of the customers
% large space of decision variables / dimensions
%
% how to avoid overwhelming of agent? output layer must be fairly large.
%
% time, energy, money, communication dimensions (and subdimensions)

%\subsection{Customer demand estimation}% \label{ssub:customer_demand_estimation}
%\label{sub:customer_demand_estimation}

The customer demand prediction belongs to the customer market space. In my implementation, I focused on the demand
prediction and skipped the tariff generation. While there are interesting approaches that can leverage machine learning
to generate numerous tariff structures, this is left for future work. 

When predicting demand, it is helpful to first perform a preliminary analysis of the structure of the demand patterns.
This has been done using Jupyter Notebooks and the work can be seen in the \texttt{notebooks} folder in the
broker-python code repository
\footnote{\url{https://github.com/pascalwhoop/broker-python/blob/master/notebooks}}.
Figure~\ref{fig:demandtimelag} shows a clear correlation between the current demand of the market and historical demands with a
delay of 12,24,36,48,... hours. This degrades however and slowly converges towards no correlation.

\begin{figure}[]
    \centering
    \includegraphics[width=0.6\linewidth]{img/demand_6.png}
    \caption{Lag Plot showing the correlation of the population usage data in relation to the time lag}
    \label{fig:demandtimelag}
\end{figure}

The analysis also showed large differences in the demand profile of different customers. Some consume several thousand
\ac {kWh} per time step while most normal consumers only consume small amounts. This however is not directly
translatable into tariff market actions because some customers are actually just a population model that simulates many
thousand individuals. These models may create contracts with a number of brokers, breaking the demand of these large
models down into several small transactions spread across tariffs.

From a technical perspective, this component has no dependencies onto the other learning components and can easily be
trained using historical data.  It is therefore a supervised learning algorithm, matching known information in timestep
$t-n$ to a prediction for the expected energy usage at timestep $t$. Because there are several games on record, the
historical realized usages are the labels for the supervised learning problem. Known information includes: Weather
forecasts, historical usages, time, tariff and customer metadata. A simplest approach and therefore a baseline to
measure against is to simply predict the customer to consume the same amount as in the previous time slot. All demand
prediction loss is measured in mean average error. A linear error is chosen because the economic impact of misjudging a
demand is also linear. When using such a prediction method as well as a -12h and -24h prediction, the results are as
seen in Figure~\ref{fig:demand_baseline}. A random prediction based on a neural network being fed random data usually
levels out at 300 to 400. This gives us a goal range (below -24h prediction) as well as a pattern to avoid which
basically equals not learning anything.

\begin{figure}[]
    \centering
    \includegraphics[width=0.8\linewidth]{img/demand_baselines.png}
    \caption{Demand baselines, -1h blue, -12h pink, -24h green}
    \label{fig:demand_baseline}
\end{figure}

If the customer models change across games (e.g. if a customer suddenly uses 10x the energy on rainy days), the learning
model will have to learn to adapt to this change. This can be achieved by letting the model both learn from historical
data initially (i.e. form the state files) and also let it learn online during the competition, based on the new
customer models.

To train a model that predicts the demand amounts of customers under various conditions, a dataset of features and
labels needs to be created. Because the model may also learn during the course of a running competition, a generator
based structure should be preferred. This means that a generator exists that creates $x, y$ pairs for the model to train
on, instead of creating a large batch of learning data ahead of the learning processing, which is otherwise a common
practice. Whenever a round completes and new information is available, the demand estimator is asked to estimate the
demand for all customers subscribed to the tariffs of the broker for the next 24 timesteps. These estimations are then
saved (i.e. they replace any previous estimations) and the wholesale component as well as other components can act on
this newly created estimations.

According to the simulation specification, the customer models generate their demand pattern based on their internal
structure, broker factors and game factors \citep[]{ketter2018powertac}. The preprocessing pipeline of the generator therefore generates
feature-label pairs that include: Customer, tariff, weather, time and demand information. The realized demand is the
label while all other components are part of the features that are used to train the model. The intuitive model class
for demand patterns prediction are \ac {RNN} due to the sequential nature of the problem \citep[]{EvalGRU2014}. However,
as will be shown later, the implementation of relatively shallow dense classic \ac {NN} also results in decent results.

\begin{figure}[h] 
    %TODO export new variant in drawio
    \centering 
    \includegraphics[width=0.8\linewidth]{img/UsageEstimator.png} 
    \caption{Demand Estimator structure} 
\label{fig:DemandEstimator} 
\end{figure}


The overall structure of the demand estimator component is shown in Figure~\ref{fig:DemandEstimator}. The model can be
both trained offline based on the state files as well as online during the competition. This is possible because in both
situations, the environment model of the agent is a continuous representation of the agents knowledge about the world.
In fact, during the state file parsing, the environment may even hold information that the agent usually cannot observe
in a competition environment. This is also the case for the demand learning, as the state files hold the demand
realizations of all customers while the server during the competition only transmits the usage realizations of the
customers that are subscribed to the agents tariffs. Regardless, this does not affect the ability to learn from the
customers usage patterns in either setting. During a competition, the agent may learn from the realized usage of
customers after each time slot is completed. Because this process may require some ressources, it is advantageous to
first perform the prediction of the subscribed customers demands for the current time slot to pass this information to
the wholesale component before training the model on the received meter readings. While the broker is waiting for the
server to process a step in the game, it can perform any learning on newly received information \footnote{The component code can be
found under \url{https://github.com/pascalwhoop/broker-python/tree/master/agent_components/demand}}.

%TODO write final model structure and plot against intuitive -24h baselines

\section{Wholesale Market}
\label{sec:wholesale_market}

To approach the wholesale trading problem, a subset of the definition of the trading problem developed by
\citet{tactexurieli2016mdp} is assumed. More specifically, the agent only concerns itself with the activities in the
wholesale market and does not act or evaluate tariff market or balancing market activities.

They defined each of the target timeslots as a \ac{MDP} with 24 states before
termination. As previously mentioned, \citet{tactexurieli2016mdp} define the entire simulation as a unified \ac{MDP}.
% each timeslot is not considered an independent \ac {MDP} anymore!
Each target timeslot is regarded as
an independent \ac{MDP}. The agent progresses through the states towards the terminal state which is the step at which
the balancing market determines an ultimate balancing charge. The reward for the agent is received at the terminal state
and is defined as the relation of the average price paid per \ac{kWh} by the agent in relation to the average market
\ac{kWh}
price for a given timestep. This removes any bias in the reward introduced by market price fluctuations. To calculate
the reward, I first calculate the average price paid by the agent per \ac{kWh} based on the realized price and quantity
$P^r_i$ and $Q^r_i$ respectively:
\begin{equation}
    \label{eq:Average price per kWh for a given target timeslot}
    %average price paid per \ac{kWh} by broker
    P^{r}_{avg} =\frac{\sum ^{1}_{i=24} P^{r}_{i} *Q^{r}_{i}}{\sum ^{1}_{i=24} Q^{r}_{i}}

    %TODO encouraging for exploration injecting into rewards

\end{equation}
This is also calculated for the market prices $P^m_{i}$ and quantities $Q^m_i$ cleared during each timeslot for the whole market and then the
relationship between the agents average costs and the average market costs is used as the reward
\begin{equation}
    %relationship between average price paid by broker and average market price for target timeslot
    R(t) = \frac{P^r_{avg}}{P^m_{avg}}
\end{equation}

\begin{markdown}

### WIP 

- adapting the wholesale environment to be  compatible to the OpenAI Gym "Environment" which allows for 

\end{markdown}


Using \ac {MDP}

\ac {MDP} is actually with infinite states but for analytical concept, its irrelevant. Important is: Continuous states,
continuous actions (with some rounding to nearest .02)

Bellman equation not applicable to continuous spaces. But it is also unique because its a directed acyclic graph (the
state transition graph) 1, 2, 3, 4, ... 24

theoretically it's a nonstationary \ac {MDP} because it's limited to 24 state transitions before termination (t-0)

DQN --> evaluate the value of a s,a pair

\ac {PPO} --> use DNN for determining what to do in a given state - maximizes "surrogate" (relation between old and new
policy) while penalizing too extensive reward estimations (to avoid extensive updates to policy)

reward is based on relative price paid in comparison to average price for given time slot.

Do I implement the env interface defined by OpenAI and let the agent subcomponent imagine it's by itself? --> allows for
A3C and many other interesting opportunities

