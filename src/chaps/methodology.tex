\section{Methodology}
First, I will perform a literature research into the fields of \ac{AI}, \ac{RL} and competitive simulations in energy
markets. In the field of AI
it's sub fields of \ac {SL}  and \ac {UL}  will be introduced. Here I will focus on the area of \ac{NN} and a form of
\emph{learning}  called Backpropagation. In the field of \ac{RL} I will focus on the \ac{MDP} framework as well as the
\ac{POMDP} subclass.  Next follows an introduction of the recent research in using \ac{NN} in \ac{RL} settings to allow
for what is now called Deep Reinforcement Learning. This field has seen tremendous success in recent research, allowing
for agents that successfully play Atari games and the game Go on superhuman levels of performance 
\citep{proximalpolicyopt, silver2016mastering}.

%After having introduced the basic research of \ac{AI} and \ac{RL}, I will summarize the state of research of Animal
%Cognition, which focuses on how animals and humans learn, act and remember in their environment. Since humans and
%animals are the only known form of intelligent life to us as of today, it is intuitive why exploring the exact workings
%of these examples might help in better understanding how to artificially create intelligence. It is also a basis of the
%thesis, as many animals show forms of social learning, concepts of teaching and learning through observation.
%TODO ref 

Following the theoretical foundations of \ac{AI},  I will introduce the concept of competitive simulations in research
and summarize the \ac{PowerTAC} competition, it's parts and how agents (called brokers in the context of \ac{PowerTAC})
make decisions. This includes an analysis of previous agents solution approaches. 

Finally, I will explain how I approached two important decision areas of brokers using recent research results using
deep learning. With this, the broker is able to learn from other agents past actions using off-policy learning
approaches by analyzing the recorded simulation sessions. 

Finally, a conclusion is drawn and the limits and weaknesses as well as recommended further research is discussed.
