\chapter{Introduction}

%\section{Motivation}
%\section{Problem Statement}
%\section{Structure}


\chapter{Background}

\section{Cognitive Science}
\subsection{Classic models of intelligent agents}
\subsection{Transactive Memory Systems}
\subsection{Community of Knowledge}



\section{Artificial Intelligence}
\subsection{Supervised Learning}
\subsubsection{Neural Networks}
\subsubsection{Backpropagation}

\subsection{Reinforcement Learning}
%\subsubsection{Passive and Active Reinforcement Learning}
\subsubsection{Policy Search}
\subsubsection{Deep Reinforcement Learning}
\subsubsection{Proximal Policy Optimization}

%\section{Game Theory}

\section{OpenAI Gym}
\section{PowerTAC}
\subsection{Zero Sum Games and Free Markets}
\subsection{Multi-Agent Competition}


\chapter{Methodology}

\section{Observational Learning for AI agents}
\section{Implementing observation through backpropagation}

\chapter{Experiment}

\section{Implementation}
\subsection{TensorFlow}

\section{Imitating locomotion in the OpenAI Gym}
\subsection{Training a normal PPO agent}
\subsection{Letting agents observe each others movements}

\section{Imitating electricity trading in the PowerTAC competition}
\subsection{PowerTAC architecture}
\subsection{PowerTAC client}
\subsection{Bridging Python and Java with Protocol Buffers}
\subsection{Training imitating agents with PowerTAC}






\chapter{Results}

\chapter{Conclusion}
