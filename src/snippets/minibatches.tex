Mini-batches refer to the random shuffling of training examples during backpropagation. While performing backpropagation for all training examples at once is significantly computing-expensive, mini-batches are a stochastic approximation towards the real gradient of the loss function and each of them gives you an approximate, although not perfect improvement in the gradient descent. However, when iterating over several batches, they converge towards the same result as if all values were combined in one backpropagation step, without the related computational cost. 
